{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 策略迭代\n",
    "## 1.游戏环境设置"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# 4x12，（11，11）是终点，最下面一行除了（3，0）以外全是不可走的地方\n",
    "#获取一个格子的状态\n",
    "def get_state(row, col):\n",
    "    if row != 3:\n",
    "        return 'ground'\n",
    "\n",
    "    if row == 3 and col == 0:\n",
    "        return 'ground'\n",
    "\n",
    "    if row == 3 and col == 11:\n",
    "        return 'terminal'\n",
    "\n",
    "    return 'trap'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定agent在每一个格子里的动作\n",
    "def move(row, col, action):\n",
    "    # 如果当前已经在陷阱或者终点，则不能执行任何动作，反馈都是0\n",
    "    if get_state(row, col) in ['trap', 'terminal']:\n",
    "        return row, col, 0\n",
    "\n",
    "    # 执行动作\n",
    "    if action == 0:   # ↑\n",
    "        row -= 1\n",
    "    elif action == 1: # ↓\n",
    "        row += 1\n",
    "    elif action == 2: # ←\n",
    "        col -= 1\n",
    "    elif action == 3: # →\n",
    "        col += 1\n",
    "\n",
    "    # 初始化reward\n",
    "    reward = -1  # 这样强迫了机器尽快结束游戏,因为每走一步都要扣一分\n",
    "\n",
    "    # 不允许走到地图外面去，撞墙扣5分\n",
    "    if row < 0 or row > 3 or col < 0 or col > 11:\n",
    "        reward = -50\n",
    "        row = max(0, min(row, 3))\n",
    "        col = max(0, min(col, 11))\n",
    "\n",
    "    # 是陷阱的话，奖励是-100\n",
    "    # 结束最好是以走到终点的形式,避免被扣100分\n",
    "    state = get_state(row, col)\n",
    "    if state == 'trap':\n",
    "        reward = -100\n",
    "    elif state == 'terminal':\n",
    "        reward = 100\n",
    "\n",
    "    return row, col, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.算法设置"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# 计算在一个状态下执行动作的分数\n",
    "def get_qsa(values, row, col, action):\n",
    "    # 在当前状态下执行动作,得到下一个状态和reward\n",
    "    next_row, next_col, reward = move(row, col, action)\n",
    "\n",
    "    # 计算下一个状态的分数,取values当中记录的分数即可,0.9是折扣因子γ\n",
    "    value = values[next_row, next_col] * 0.9\n",
    "\n",
    "    # 如果下个状态是陷阱,则下一个状态的分数是0\n",
    "    if get_state(next_row, next_col) in ['trap', 'terminal']:\n",
    "        value = 0\n",
    "\n",
    "    # 动作的分数本身就是reward,加上下一个状态的分数\n",
    "    return value + reward"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "由于策略迭代是无穷的，所以这里使用的截断策略迭代算法（Truncated policy iteration algorithm）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def policyIteration(values,pi):\n",
    "    # 训练10轮\n",
    "    for _ in range(10):\n",
    "\n",
    "        # 策略评估\n",
    "        # 轨迹长度为100\n",
    "        for _ in range(100):\n",
    "            #初始化一个新的values,重新评估所有格子的分数\n",
    "            new_values = np.zeros([4, 12])\n",
    "            # 遍历所有格子\n",
    "            for row in range(4):\n",
    "                for col in range(12):\n",
    "\n",
    "                    #计算当前格子4个动作分别的分数\n",
    "                    action_value = np.zeros(4)\n",
    "\n",
    "                    #遍历所有动作\n",
    "                    for action in range(4):\n",
    "                        action_value[action] = get_qsa(values, row, col, action)\n",
    "\n",
    "                    #每个动作的分数和它的概率相乘\n",
    "                    action_value *= pi[row, col]\n",
    "\n",
    "                    #最后这个格子的分数,等于该格子下所有动作的分数求和\n",
    "                    new_values[row, col] = np.sum(action_value)\n",
    "\n",
    "            values = new_values\n",
    "\n",
    "        # 策略提升\n",
    "        # 遍历所有格子\n",
    "        new_pi = np.zeros([4, 12, 4])\n",
    "        for row in range(4):\n",
    "            for col in range(12):\n",
    "                # 计算当前格子4个动作分别的分数\n",
    "                action_values = np.zeros(4)\n",
    "                maxActionIdx = 0\n",
    "                # 遍历所有动作\n",
    "                for action in range(4):\n",
    "                    action_values[action] = get_qsa(values, row, col, action)\n",
    "                # 将动作值最大的动作作为下一步\n",
    "                #计算当前状态下，达到最大分数的动作有几个\n",
    "                count = (action_values == action_values.max()).sum()\n",
    "                # 让该动作的概率为1，其他动作的概率为0\n",
    "                for action in range(4):\n",
    "                    if action_values[action] == action_values.max():\n",
    "                        new_pi[row, col, action] = 1 / count\n",
    "                    else:\n",
    "                        new_pi[row, col, action] = 0\n",
    "                # 这里如果直接取一个最大值而不是在相同最大值中随机取一个动作，会导致agent走向错误的路线\n",
    "\n",
    "\n",
    "        pi = new_pi\n",
    "\n",
    "    return values, pi"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.游戏初始化与训练"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 17.96052411,  21.06724901,  24.51916557,  28.35462841,\n         32.61625379,  37.3513931 ,  42.612659  ,  48.45851   ,\n         54.9539    ,  62.171     ,  70.19      ,  79.1       ],\n       [ 21.06724901,  24.51916557,  28.35462841,  32.61625379,\n         37.3513931 ,  42.612659  ,  48.45851   ,  54.9539    ,\n         62.171     ,  70.19      ,  79.1       ,  89.        ],\n       [ 24.51916557,  28.35462841,  32.61625379,  37.3513931 ,\n         42.612659  ,  48.45851   ,  54.9539    ,  62.171     ,\n         70.19      ,  79.1       ,  89.        , 100.        ],\n       [ 21.06724901,   0.        ,   0.        ,   0.        ,\n          0.        ,   0.        ,   0.        ,   0.        ,\n          0.        ,   0.        ,   0.        ,   0.        ]])"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#初始化每个格子的价值\n",
    "values = np.zeros([4, 12])\n",
    "\n",
    "#初始化每个格子下采用动作的概率\n",
    "pi = np.ones([4, 12, 4]) * 0.25\n",
    "#循环迭代策略评估和策略提升，寻找最优解\n",
    "\n",
    "values, pi = policyIteration(values,pi)\n",
    "values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.训练结果可视化"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#打印游戏，方便测试\n",
    "def show(row, col, action):\n",
    "    graph = [\n",
    "        '□', '□', '□', '□', '□', '□', '□', '□', '□', '□', '□', '□', '□', '□',\n",
    "        '□', '□', '□', '□', '□', '□', '□', '□', '□', '□', '□', '□', '□', '□',\n",
    "        '□', '□', '□', '□', '□', '□', '□', '□', '□', '○', '○', '○', '○', '○',\n",
    "        '○', '○', '○', '○', '○', '❤'\n",
    "    ]\n",
    "\n",
    "    action = {0: '↑', 1: '↓', 2: '←', 3: '→'}[action]\n",
    "\n",
    "    graph[row * 12 + col] = action\n",
    "\n",
    "    graph = ''.join(graph)\n",
    "\n",
    "    for i in range(0, 4 * 12, 12):\n",
    "        print(graph[i:i + 12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import time\n",
    "\n",
    "\n",
    "def test():\n",
    "    #起点在0,0\n",
    "    row = 0\n",
    "    col = 0\n",
    "\n",
    "    #最多玩N步\n",
    "    for _ in range(200):\n",
    "\n",
    "        #选择一个动作\n",
    "        action = np.random.choice(np.arange(4), size=1, p=pi[row, col])[0]\n",
    "\n",
    "        #打印这个动作\n",
    "        display.clear_output(wait=True)\n",
    "        time.sleep(0.3)\n",
    "        show(row, col, action)\n",
    "\n",
    "        #执行动作\n",
    "        row, col, reward = move(row, col, action)\n",
    "\n",
    "        #获取当前状态，如果状态是终点或者掉陷阱则终止\n",
    "        if get_state(row, col) in ['trap', 'terminal']:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "□□□□□□□□□□□□\n",
      "□□□□□□□□□□□□\n",
      "□□□□□□□□□□□↓\n",
      "□○○○○○○○○○○❤\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
