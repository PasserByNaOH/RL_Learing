{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 贝尔曼公式"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#状态转移概率矩阵\n",
    "P = np.array([\n",
    "    [0.9, 0.1, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.5, 0.0, 0.5, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.6, 0.0, 0.4],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.3, 0.7],\n",
    "    [0.0, 0.2, 0.3, 0.5, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "])\n",
    "\n",
    "#到达每一个状态的奖励\n",
    "R = np.array([-1, -2, -2, 10, 1, 0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$G_t$为一个轨迹的discounted return\n",
    "$$\n",
    "\\begin{aligned}\n",
    "G_t &= R_{t+1}+\\gamma R_{t+2} + \\gamma^2 R_{t+3} + …… \\\\\n",
    "\t&= R_{t+1}+\\gamma (R_{t+2} + \\gamma R_{t+3} + ……) \\\\\n",
    "\t&= R_{t+1} + \\gamma G_{t+1}\n",
    "\\end{aligned}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#给定一条序列,计算回报,计算的就是discounted return\n",
    "def value_by_chain(chain):\n",
    "    s = 0\n",
    "    for i, c in enumerate(chain):\n",
    "        #给每一步的反馈做一个系数,随着步数往前衰减\n",
    "        s += R[c] * 0.5**i\n",
    "\n",
    "    #最终的反馈是所有步数衰减后的求和\n",
    "    return s\n",
    "value_by_chain(np.array([0, 1, 2, 5]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "贝尔曼公式为\n",
    "- 代数形式\n",
    "描述了不同状态值之间的关系，对于所有状态都成立，即一个状态对应一个式子\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    v_\\pi(s)\n",
    "    &= \\mathbb{E}[R_{t+1}|S_t=s] +\\gamma\\mathbb{E}[G_{t+1}|S_t=s], \\\\\n",
    "    &= \\underbrace{\\sum_{a}\\pi(a|s)\\sum_r p(r|s,a)r}_{当前奖励的平均值} +\\underbrace{\\gamma \\sum_a \\pi(a|s)\\sum_{s'}p(s'|s,a)v_{\\pi}(s')}_{未来奖励的平均值}\\\\\n",
    "    &= \t\\sum_{a}\\pi(a|s) \\left[ \\sum_{r}p(r|s,a)r+\\gamma\\sum_{s'}p(s’|s,a)v_{\\pi}(s') \\right], \\forall s\\in S.\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n",
    "- 矩阵形式\n",
    "    本次实验的贝尔曼公式（矩阵形式）\n",
    "    $$\n",
    "        v_{\\pi}(s_i)=r_{\\pi}(s_i)+\\gamma \\sum_{s_j}p_{\\pi}(s_j|s_i)v_{\\pi}(s_j)\n",
    "    $$\n",
    "    $$\n",
    "    \\underbrace{v_{\\pi}(s)}_{value[i]} = \\underbrace{r(s)}_{R[i]} + \\underbrace{γP_{\\pi}v_{\\pi}(s')}_{γ=0.5,\\ 0.5 * P[i].dot(value)}\n",
    "    $$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#梯度下降法计算贝尔曼矩阵,近似值\n",
    "# 由于对所有状态都成立，所以要遍历所有状态\n",
    "def get_bellman():\n",
    "    #初始化values\n",
    "    value = np.ones([6])\n",
    "    # 迭代200次\n",
    "    for _ in range(200):\n",
    "        # 6个动作\n",
    "        for i in range(6):\n",
    "            #每一行的概率和它对应的value相乘，乘以gamma，然后和奖励相加\n",
    "            #反复计算，就收敛到了贝尔曼方程矩阵\n",
    "            value[i] = R[i] + 0.5 * P[i].dot(value)\n",
    "\n",
    "    return value\n",
    "get_bellman()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#解析解贝尔曼方程矩阵，真实值\n",
    "def get_bellman():\n",
    "    mat = np.eye(*P.shape)\n",
    "    mat -= 0.5 * P\n",
    "    mat = np.linalg.inv(mat)\n",
    "\n",
    "    return mat.dot(R)\n",
    "\n",
    "\n",
    "get_bellman()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 贝尔曼最优公式\n",
    "贝尔曼最优公式是一个理论基础，它需要与特定的算法结合，才能实际应用于解决问题。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# 构造一个网格环境(3x3)\n",
    "def EnvSetting(row, col):\n",
    "    if row == 0 and col == 1:\n",
    "        return 'trap'\n",
    "    if row == 2 and col == 0:\n",
    "        return 'trap'\n",
    "    if row == 2 and col ==2:\n",
    "        return 'terminal'\n",
    "    return 'ground'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# 设值agent在每一个格子里的动作\n",
    "def move(row, col, action):\n",
    "    #如果当前已经在陷阱或者终点，则不能执行任何动作，反馈都是0\n",
    "    if EnvSetting(row, col) in ['trap', 'terminal']:\n",
    "        return row, col, 0\n",
    "    #↑\n",
    "    if action == 0:\n",
    "        row -= 1\n",
    "    #↓\n",
    "    if action == 1:\n",
    "        row += 1\n",
    "    #←\n",
    "    if action == 2:\n",
    "        col -= 1\n",
    "    #→\n",
    "    if action == 3:\n",
    "        col += 1\n",
    "    #不允许走到地图外面去\n",
    "    row = max(0, row)\n",
    "    row = min(2, row)\n",
    "    col = max(0, col)\n",
    "    col = min(2, col)\n",
    "\n",
    "    #是陷阱的话，奖励是-100，否则都是-1\n",
    "    #这样强迫了机器尽快结束游戏,因为每走一步都要扣一分\n",
    "    #结束最好是以走到终点的形式,避免被扣100分\n",
    "    reward = -1\n",
    "    if EnvSetting(row, col) == 'trap':\n",
    "        reward = -100\n",
    "\n",
    "    return row, col, reward"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#初始化每个格子的价值\n",
    "values = np.zeros([3, 3])\n",
    "\n",
    "#初始化每个格子下采用动作的概率\n",
    "pi = np.ones([3, 3, 4]) * 0.25"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "#值评估,返回更新后的状态值矩阵\n",
    "def valuesEvaluation():\n",
    "\n",
    "    #初始化一个新的values,重新评估所有格子的分数\n",
    "    new_values = np.zeros([3, 3])\n",
    "    # 0.9是折扣因子γ\n",
    "    gamma = 0.9\n",
    "    #遍历所有格子\n",
    "    for row in range(3):\n",
    "        for col in range(3):\n",
    "            # 如果下个状态是终点或者陷阱,就跳过计算\n",
    "            if EnvSetting(row, col) in ['trap', 'terminal']:\n",
    "                continue\n",
    "\n",
    "            # 应用贝尔曼最优公式得出最佳价值\n",
    "            maxValue = -float('inf')\n",
    "            for action in range(4):\n",
    "                next_row, next_col, reward = move(row, col, action)\n",
    "                # 计算当前状态下执行动作后的预期价值\n",
    "                value = reward + gamma * values[next_row, next_col]\n",
    "                maxValue = max(maxValue, value)\n",
    "            new_values[row, col] = maxValue\n",
    "\n",
    "    return new_values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# 策略更新\n",
    "def policyUpdate():\n",
    "    #重新初始化每个格子下采用动作的概率,重新评估\n",
    "    new_pi = np.zeros([3, 3, 4])\n",
    "    # 0.9是折扣因子γ\n",
    "    gamma = 0.9\n",
    "    #遍历所有格子\n",
    "    for row in range(3):\n",
    "        for col in range(3):\n",
    "\n",
    "            if EnvSetting(row, col) in ['trap', 'terminal']:\n",
    "                continue\n",
    "\n",
    "            #计算当前格子4个动作分别的分数\n",
    "            action_value = np.zeros(4)\n",
    "\n",
    "            #遍历所有动作\n",
    "            for action in range(4):\n",
    "                next_row, next_col, reward = move(row, col, action)\n",
    "                # 计算当前状态下执行动作后的预期价值\n",
    "                action_value[action] = reward + gamma * values[next_row, next_col]\n",
    "\n",
    "            #计算当前状态下，达到最大分数的动作有几个\n",
    "            count = (action_value == action_value.max()).sum()\n",
    "\n",
    "            #让这些动作均分概率\n",
    "            for action in range(4):\n",
    "                if action_value[action] == action_value.max():\n",
    "                    new_pi[row, col, action] = 1 / count\n",
    "                else:\n",
    "                    new_pi[row, col, action] = 0\n",
    "\n",
    "    return new_pi"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[-3.439,  0.   , -1.9  ],\n        [-2.71 , -1.9  , -1.   ],\n        [ 0.   , -1.   ,  0.   ]]),\n array([[[0. , 1. , 0. , 0. ],\n         [0. , 0. , 0. , 0. ],\n         [0. , 1. , 0. , 0. ]],\n \n        [[0. , 0. , 0. , 1. ],\n         [0. , 0.5, 0. , 0.5],\n         [0. , 1. , 0. , 0. ]],\n \n        [[0. , 0. , 0. , 0. ],\n         [0. , 0. , 0. , 1. ],\n         [0. , 0. , 0. , 0. ]]]))"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#循环迭代策略评估和策略提升，寻找最优解\n",
    "for _ in range(10):\n",
    "    for _ in range(100):\n",
    "        values = valuesEvaluation()\n",
    "    pi = policyUpdate()\n",
    "\n",
    "values,pi"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "□□○\n",
      "□↑□\n",
      "○□❤\n"
     ]
    }
   ],
   "source": [
    "# 打印游戏，方便测试\n",
    "def show(row, col, action):\n",
    "    graph = [\n",
    "        '□', '□', '○', '□', '□', '□', '○', '□', '❤'\n",
    "    ]\n",
    "\n",
    "    action = {0: '↑', 1: '↓', 2: '←', 3: '→'}[action]\n",
    "\n",
    "    graph[row * 3 + col] = action\n",
    "\n",
    "    graph = ''.join(graph)\n",
    "\n",
    "    for i in range(0, 3 * 3, 3):\n",
    "        print(graph[i:i + 3])\n",
    "\n",
    "\n",
    "show(1, 1, 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import time\n",
    "\n",
    "\n",
    "def test():\n",
    "    #起点在0,0\n",
    "    row = 0\n",
    "    col = 0\n",
    "\n",
    "    #最多玩N步\n",
    "    for _ in range(200):\n",
    "\n",
    "        #选择一个动作\n",
    "        action = np.random.choice(np.arange(4), size=1, p=pi[row, col])[0]\n",
    "\n",
    "        #打印这个动作\n",
    "        display.clear_output(wait=True)\n",
    "        time.sleep(0.5)\n",
    "        show(row, col, action)\n",
    "\n",
    "        #执行动作\n",
    "        row, col, reward = move(row, col, action)\n",
    "\n",
    "        #获取当前状态，如果状态是终点或者掉陷阱则终止\n",
    "        if EnvSetting(row, col) in ['trap', 'terminal']:\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "□□○\n",
      "□□□\n",
      "○→❤\n"
     ]
    }
   ],
   "source": [
    "test()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
