{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入游戏库\n",
    "import gym\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义游戏环境\n",
    "class MyWrapper(gym.Wrapper):\n",
    "\n",
    "    def __init__(self):\n",
    "        #is_slippery控制会不会滑\n",
    "        env = gym.make('FrozenLake-v1',\n",
    "                       render_mode='rgb_array',\n",
    "                       is_slippery=False)\n",
    "\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "\n",
    "    def reset(self):\n",
    "        state, _ = self.env.reset()\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        over = terminated or truncated\n",
    "\n",
    "        #走一步扣一分,逼迫机器人尽快结束游戏\n",
    "        if not over:\n",
    "            reward = -1\n",
    "\n",
    "        #掉坑扣100分，游戏它本身规则就是掉坑就停止\n",
    "        if over and reward == 0:\n",
    "            reward = -100\n",
    "\n",
    "        return state, reward, over\n",
    "\n",
    "    #打印游戏图像\n",
    "    def show(self):\n",
    "        from matplotlib import pyplot as plt\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.imshow(self.env.render())\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "env = MyWrapper()\n",
    "\n",
    "env.reset()\n",
    "\n",
    "env.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化action value表（Q表）\n",
    "import numpy as np\n",
    "Q = np.zeros((16,4))\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import random\n",
    "\n",
    "\n",
    "#玩一局游戏并记录数据\n",
    "def play(show=False):\n",
    "    data = []\n",
    "    reward_sum = 0\n",
    "    # 重置所有状态\n",
    "    state = env.reset()\n",
    "    # 停止标志位重置\n",
    "    over = False\n",
    "    while not over:\n",
    "        # 以下实现了ε-贪婪策略\n",
    "        # 选择当前状态最大action value的动作\n",
    "        action = Q[state].argmax()\n",
    "        # 以 10% 的概率，随机选择一个动作action，用于探索新的动作策略\n",
    "        if random.random() < 0.1:\n",
    "            # 随机选一个动作\n",
    "            action = env.action_space.sample()\n",
    "        #  执行选择的动作 action，获取新的状态 next_state、奖励 reward 和是否结束标志 over\n",
    "        next_state, reward, over = env.step(action)\n",
    "        # 记录数据\n",
    "        data.append((state, action, reward, next_state, over))\n",
    "        # 获得奖励\n",
    "        reward_sum += reward\n",
    "        # 状态更新\n",
    "        state = next_state\n",
    "        # 如果 show 参数为 True，则调用环境的 show() 方法显示游戏画面\n",
    "        if show:\n",
    "            display.clear_output(wait=True)\n",
    "            env.show()\n",
    "\n",
    "    return data, reward_sum\n",
    "\n",
    "\n",
    "play(True)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据池\n",
    "# 用于存储 Agent 在游戏过程中收集的经验数据，这些数据将被用于训练 Agent 的策略。\n",
    "class Pool:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pool = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pool)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.pool[i]\n",
    "\n",
    "    #更新动作池\n",
    "    def update(self):\n",
    "        #每次更新不少于200条新数据\n",
    "        old_len = len(self.pool)\n",
    "        while len(pool) - old_len < 200:\n",
    "            # 调用play玩一局游戏，并将获取的游戏数据 play()[0]（游戏轨迹）追加到数据池中\n",
    "            self.pool.extend(play()[0])\n",
    "\n",
    "        #只保留最新的条10000数据\n",
    "        self.pool = self.pool[-1_0000:]\n",
    "\n",
    "    #获取一批数据样本\n",
    "    def sample(self):\n",
    "        return random.choice(self.pool)\n",
    "\n",
    "\n",
    "pool = Pool()\n",
    "pool.update()\n",
    "\n",
    "len(pool), pool[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练\n",
    "def train():\n",
    "    #共更新N轮数据\n",
    "    for epoch in range(1000):\n",
    "        pool.update()\n",
    "\n",
    "        #每次更新数据后,训练N次\n",
    "        for i in range(200):\n",
    "\n",
    "            #随机抽一条数据\n",
    "            state, action, reward, next_state, over = pool.sample()\n",
    "\n",
    "            #Q矩阵当前估计的state下action的价值\n",
    "            value = Q[state, action]\n",
    "\n",
    "            #实际玩了之后得到的reward+下一个状态的价值*0.9\n",
    "            target = reward + Q[next_state].max() * 0.9\n",
    "\n",
    "            #value和target应该是相等的,说明Q矩阵的评估准确\n",
    "            #如果有误差,则应该以target为准更新Q表,修正它的偏差\n",
    "            #这就是TD误差,指评估值之间的偏差,以实际成分高的评估为准进行修正\n",
    "            update = (target - value) * 0.1\n",
    "\n",
    "            #更新Q表\n",
    "            Q[state, action] += update\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(epoch, len(pool), play()[-1])\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(True)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
